# Paper-Reading
记录论文阅读的点滴
各位老师大家好，我今天主要来介绍一下3D目标检测，

首先来介绍一下背景

背景的话，主要就是来介绍一下平面的目标检测算法，目标检测算法是建立在图像分类算法上的，图像分类的要求是对输入图像进行正确的分类，通常输入的图像大小也比较小，而目标检测主要是在一张包含多个物体的图像中，不仅要识别出图像的类别，更要识别出物体的位置信息。

业界在处理图像分类问题时看成了一个分类的问题，而处理位置信息时则有看成了一个回归问题

这个表显示出目标检测的发展历史

可以看到，图像分类领域率先在2012年的AlexNet就开始应用了深度学习的方法，而目标检测是在两年后2014年才应用了深度学习的方法，目标检测一共有两种实现方法，一种是以RCNN系列为代表的two-stage方法，在这个表中用红色标志出来，它的主要思想是首先从输入图片中提取一系列的候选区域，再把这些候选区域输入到一个训练好的神经网络中进行图像分类和位置修正。第二种方法是以yolo系列为代表的one-stage的方法，在表中用蓝色标志了出来，这种方法的主要的思想是把目标检测问题看成一个端到端的完整网络，输入进图片，输出物体的位置和相关种类。one-stage方法的代表算法还有SSD算法。

这张图是一个更为详细的目标检测发展历史

总体来说，这两种目标检测的方法各有优劣，总体来说two-stage的方法有更为精确的结果，运行速度却不尽如人意，而one-stage的方法虽然牺牲了准确度，但是速度却领先很多。

下面介绍下在目标检测网络中的评价标准，在3D的目标检测中，用到的也是相关的评价标准，评价标准一共有两个一个是精度评价标准mAP和速度评价标准FPS

首先来说精度评价标准mAP，在计算精度评价标准，首先是IOU的计算，假设红色框是真实标签，绿色框是预测值，那么它们的重合程度则反映了预测值的准确性，一般认为IOU大于0.5时即视为预测正确。一个网络会预测出很多包围框来，而每一个包围框都会有一个IOU值，在求得这些IOU值之后，需要计算出精确率和召回率，精确率的定义是所有预测为正的包围框中预测正确的个数有多少，预测为正指的是预测这个包围框包含某类物体，而召回率的定义是在所有应该预测为正的包围框中有多少预测了出来，更为直观的理解精确度是IOU框的交集与预测框的比值，而召回率是IOU框的交际与真实框的比值，IOU则是交集与并集的比值，在明确精确度与召回率的定义之后，我们每计算一个包围框的IOU的时候都要更新精确度与召回率，最终得到PR曲线，而平均精度AP则是计算这个曲线包围的面积，mAP是指所有类别的AP取平均，而值得注意的一点是，我们在计算PR曲线面积的时候，采用了一种向上补齐的办法方便计算，即随着召回率的增加应该保证曲线是非增的。

再说速度评价标准FPS，这个很好理解，是指每秒钟可以检测的图像的个数，FPS越高，则意味着网络的速度性能越好，这张图是一个主流网络的FPS，mAP对比图



接下来讲3D的目标检测，首先介绍前言，再对比上面的one-stage路线和two-stage路线，分别介绍两个网络，它们分别对应着Yolo和RCNN的变形，最后是总结

首先介绍前言

首先介绍一下3D目标检测的优势，核心点，可以获得更多的信息。

举例说明，如果在无人驾驶、机器人、增强现实的应用场景下，普通2D检测并不能提供感知环境所需要的全部信息，2D检测仅能提供目标物体在二维图片中的位置和对应类别的置信度，但是在真实的三维世界中，物体都是有三维形状的，大部分应用都需要有目标物体的长宽高还有偏转角等信息。例如下图Fig.1中，在自动驾驶场景下，需要从图像中提供目标物体三维大小及旋转角度等指标，在鸟瞰投影的信息对于后续自动驾驶场景中的路径规划和控制具有至关重要的作用。

而如果从传感器进行分类的话，基本上会用到两种传感器，相机和激光雷达，相机有单目相机，只能拍摄平面的照片，双目相机可以添加深度的测量，拍摄RGBD照片，激光雷达则是根据距离周围障碍的距离返回一张点云图，相机的造价低廉但精确性不够，目前网络都在用相机作为传感器，而激光雷达虽然精度更高，但是价格非常高，对计算能力的要求也比较高，不过最近的雷达有降价的趋势，后面要介绍的两个模型都是用激光雷达数据作为输入的，除此之外，有很多网络是将雷达和相机结合起来作为输入，以期达到更好的效果。

接下来是来介绍yolo的3D变换模型，其实按照正常的思维，应该先来介绍RCNN的变形，但是YOLO的变形比较便于理解，而且有一些参数也会用在后面的讲解中，所以先来介绍YOLO的变形

首先看一下YOLO的3D变形，首先需要改变的就是输出值，前面已将说到了yolo是一个one-stage的算法，整个网络都可以看成是一个回归网络，最后的输出值肯定也是一些数据，

yolo2d中输出的数据是，到了3d中的数据就变成了

然后介绍图像的预处理，前面已经说到了，普通的yolo2d是以单目的RGB图片作为输入的，直接对图片进行卷积，而3D是对于激光雷达的点云图进行运算的，所以再数据处理上面是截然不同的，需要一种新的预处理的方式

点云图需要变化，变化成608 * 608 * 2的一张立体图

•The point cloud was projected in 2D space as a bird view grid map with a resolution of 0.1m per pixel

•

•The range represented from the LiDAR space is 30.4 meters to right and 30.4 meters to the left, and 60.8 meters forward. 

•

•Using this range with the resolution of 0.1 results in an input shape of 608x608 per channel.



•The height in the LiDAR space is between +2m and -2m, and scaled to be from 0 to 255 to be represented as pixel values

 

为什么是608呢？

为什么是2呢？两个通道

处理了点云图之后，将处理的结果输入到网络中进行卷积运算，这篇论文是2018年发表的，当时还没有yolov3，所以借鉴的大部分都是yolov2的网络，网络结构基本上都是相同的，不同点只在4点，

1是在改变了一层最大池化层的大小，可以在最后得到更大的输出特征图，这种变化对于捕捉小物体有很大的好处

2第二个是yolov2中用到了一层跳跃结构，有点模仿了resnet模型，但是该模型去掉了这层跳跃结构，原因是在实践中发现，如果没有这层跳跃结构会得到更好的准确度

3是在最终的损失函数中添加了z坐标，高度和朝向角的信息

4是输入层做了改变，前面已经介绍过了，是608 * 608 * 2的输入特征图

那么对于转向角的损失函数计算是什么样子的呢

归一化Π，和±1

s²是什么意思，一个是有s²这么多个各自进行物体的预测

B是每个格子预测的物体个数

l是判断这个格子这里有没有物体

坐标的预测变化是什么样子的呢，从二维变化过来的

做一个维度的变换，由于是回归问题，输出的结果必须做一个统一化，用sigmoid函数标到0至1以内，而长和宽则是用一个提前确定好的anchorbox相比，得到相关的倍数

最后是loss一行一行介绍

再有实验结果

这是对这三类物体，在不同的IOU的阈值上面计算的平均精度，可以看到IOU是0.5的时候，精度基本都很高

然后再来介绍一下PV-RCNN的相关网络

首先是对PV-RCNN的一个总述，PV-RCNN这个名字的全程是PointVoxel-RCNN，是在RCNN网络的基础之上，整合了voxel-based的算法和point-based的算法的优势，设计了一个新的网络

grid-based的算法的基本思想是这样的

为了解决点云的不规则数据格式问题，现有的研究大多将点云投影到规则网格上，由二维或三维CNN处理。先锋工作MV3D[1]将点云投影到2D鸟瞰网格，并放置许多预先确定的3D锚来生成3D边界框，并且开发了更好的多传感器融合策略，还有其他一些工作[27，41]将点云划分为3D体素，由3D-CNN处理，并且引入3D稀疏卷积[5]以有效地处理3D体素。还可以使用多个检测头，而[26]探索对象部件位置以提高性能。这些基于网格的方法通常对于精确的3D方案生成是有效的，但是接受域受到2D/3D卷积的核大小的限制。

总体来说v方法，是将不规则的点云图像转换成3D提像素的规则表达，或者2D的服饰图

优点缺点

P方法：

基于点Point-based的三维目标检测方法。FPointNet[22]首先提出应用PointNet[23，24]从基于2D图像边界框的裁剪点云进行3D检测。PointRCNN[25]直接从整个点云生成3D建议，而不是仅使用点云进行3D检测的2D图像，另外也有一些工作提出了稀疏到密集的策略，以更好地解决建议问题。还有一些工作提出了hough投票策略以更好地进行对象特征分组。这些基于点的方法主要基于点网系列，尤其是集合抽象操作[24]，它为点云特征学习提供了灵活的接受域。

总体来说，

基于点的这种算法是直接从点云图中提取有关信息，而不做一个2D的转换

优点缺点

所以本文是结合这两种方法的优点提出了这种新的模型，这里作者的做法是：在每一个3D proposals内平均的采样一些Grid-point，然后再通过P2的FPS最远点采样的方法得到该Grid_point周围的点，再通过结合去进一步refine最后的proposals

具体来说是

- 因此，作者采用两阶段的方法去更好的结合上述的两种算法的优点。

> （1） 第一阶段为：“voxel-to-keypoint scene encoding step ”，这一步是提出proposals，作者首先对整个场景采用voxel的方法进行特征提取，同时采取一支分支对场景采用point的FPS采样，然后检索得到多尺度的voxel的特征，如下的表示。这样实际上仅仅是采用了voxel的特征，但是表示在key-point身上。

（2）第二阶段为‘keypoint-to-grid RoI feature abstraction’：这一步骤，作者提出了一个新的RoI-grid pooling module，该模块将上一步骤的keypoints的特征和RoI-grid points特特征融合（keypoints和RoI-grid points是什么内容后续会讲到）

然后就是来具体看一看

三个部分组成最后的网络，

第一部分略

第二部分

- Keypoints Sampling

> 采用FPS，对KITTI数据集的关键点个数为2048，对waymo数据集为4096个点。用于代表整个场景的特征信息。

- Voxel Set Abstraction Module

> 作者自行设计了Voxel Set Abstraction (VSA) module这样的一个模块，这个模块的作用是将keypoint周围非空的voxel特征采集出来结合在一起，原文用了很多数学表达，含义大致如此。
>
> 具体的情节是这样的
>
> 首先是对于第k层卷积层来说，我们假设F这个集合中是所有特征点的特征信息，V这个集合中是这些关键点位置信息，那么再对于刚才提取出来的所有的关键点而言，假设第i个关键点pi，找出与该关键点相关的一个集合si，这个集合满足三个条件，那么再对si进行函数变换，函数变换s表示对si随机取样，g表示是一层网络卷积层，max是最大池化操作，得到新的特征，再做两个拼接，得到编码表达

- Extended VSA Module

> 进一步的在二维上，采用的是双线性插值得到关键点投影到BEV上的特征。最终的特征将有三部分组成，分别是来自voxel语义信息*fipv*, 来自原始点云的特征信息*firaw*(作者说这一部分信息是为了弥补之前在voxel化时丢失的信息)，来自BEV的高级信息*fibev*.

- Predicted Keypoint Weighting.

> （1）上述的特征融合实际上都是为了进一步的refine做准备，第一阶段的proposals实际上是由voxel-based的方法提出来的，这一步 Keypoint Weighting的工作是为了给来自背景和前景的关键点一个不一样的权重，使得前景对refine的贡献更大。
> （2）为了做这样的一个工作，作者设计了如下的额为的网络结构。这里面的Label对应的是是否在gt内，采用fcoal_loss。

第三部分

这就是作者提出的第二阶段，refinement，前文提到通过3D稀疏卷积处理voxel已经得到了比较好的精度的proposals，但是多尺度的keypoint的特征是为了进一步精细化处理。因此作者在这个阶段提出了**keypoint-to-grid RoI feature abstraction**模块。如下：

> （1）从该模块名称和图就可以看得出来，作者是想通过将key-point的特征整合到grid-point中去，并且也采用了multi-scale的策略。作者在每个proposals中都采样6×6×6个grid points.
> （2）首先确定每一个grid-point的一个半径下的近邻，然后再用一个pointnet模块将特征整合为grid_point的特征，这里会采用多个scale的特征融合手段。
> （3）得到了所有的grid-point的点特征后，作者采用两层的MLP得到256维度的proposals的特征。

 第三部分第二小部分

这两层的MLP网络是loss一共包含两部分，第一部分是对置信系数的损失函数做出整理，这部分用Liou来表示，iou采用的是普通的交叉熵函数来确定，而对于包围框的精细化预测采用的是L1loss





最后是整个网络的损失函数

RPN loss

- keypoint seg loss也就是前背景关键点的权重loss。
- refinement loss 定义如下：
- 

这里的两部分loss第一个置信度LOSS也就是前文提出的LOSS，后面的SmoothL1 LOSS和以前的一样。
